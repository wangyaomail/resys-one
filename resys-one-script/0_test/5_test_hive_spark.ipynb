{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用pyspark写hive\n",
    "本质上还是写hbase，但是pyspark直接访问hbase较为麻烦，因此不如使用hive外链表\n",
    "需要准备这两个jar放到spark的jars中：\n",
    "spark-hive_2.12-2.4.5.jar：\n",
    "https://search.maven.org/artifact/org.apache.spark/spark-hive_2.12/2.4.5/jar\n",
    "hive-exec-1.2.1.spark2.jar:\n",
    "https://search.maven.org/artifact/org.spark-project.hive/hive-exec/1.2.1.spark2/jar\n",
    "然后将hive和hbase目录下的lib文件夹内的所有包都复制到spark的jars中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/03 22:42:27 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7f728b666a50>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://zzti:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>hive_test</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import  SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"yarn\")\n",
    "conf.setAppName(\"hive_test\")\n",
    "#conf.set(\"spark.yarn.jars\", \"/libs/spark/*.jar\")\n",
    "ss = SparkSession.builder\\\n",
    "    .config(conf=conf)\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "ss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 18:41:29 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "21/12/01 18:41:29 WARN metastore.ObjectStore: Failed to get database hive_test, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建数据库\n",
    "ss.sql(\"create database hive_test\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                  id|              string|   null|\n",
      "|               value|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|           hive_test|       |\n",
      "|               Table|                test|       |\n",
      "|               Owner|                zzti|       |\n",
      "|        Created Time|Wed Dec 01 19:05:...|       |\n",
      "|         Last Access|Thu Jan 01 08:00:...|       |\n",
      "|          Created By|         Spark 2.4.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|                hive|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|            Location|file:/data/zzti/p...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[field.delim=,, s...|       |\n",
      "|  Partition Provider|             Catalog|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 19:05:13 WARN metastore.HiveMetaStore: Location: file:/data/zzti/proj/spark-warehouse/hive_test.db/test specified for non-external table:test\n"
     ]
    }
   ],
   "source": [
    "# 创建表\n",
    "ss.sql(\"create table hive_test.test(id varchar(20),value varchar(20)) row format delimited fields terminated by ',' \")\n",
    "ss.sql(\"desc formatted hive_test.test\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 添加数据\n",
    "ss.createDataFrame(zip(range(00,70),range(20,100)),['id','value']).write\\\n",
    "    .format('hive').insertInto(\"hive_test.test\",overwrite=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "| 50|   20|\n",
      "| 51|   21|\n",
      "| 52|   22|\n",
      "| 53|   23|\n",
      "| 54|   24|\n",
      "| 55|   25|\n",
      "| 56|   26|\n",
      "| 57|   27|\n",
      "| 58|   28|\n",
      "| 59|   29|\n",
      "| 60|   30|\n",
      "| 61|   31|\n",
      "| 62|   32|\n",
      "| 63|   33|\n",
      "| 64|   34|\n",
      "| 65|   35|\n",
      "| 66|   36|\n",
      "| 67|   37|\n",
      "| 68|   38|\n",
      "| 69|   39|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|       90|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查询数据\n",
    "ss.sql(\"select * from hive_test.test\").show()\n",
    "ss.sql(\"select count(id) from hive_test.test\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<thriftpy2.thrift.TClient at 0x7f607f059890>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用happyhbase连接hbase，需要提前启动hbase的thrift服务\n",
    "import happybase\n",
    "conn = happybase.Connection('192.168.17.150', autoconnect=True)\n",
    "conn.open()\n",
    "conn.client"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 创建HBase外接表\n",
    "conn.create_table(\"hive_test_hbase\", {'d': dict(max_versions=1, block_cache_enabled=False)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " create external table if not exists hive_test.hive_test_hbase\n",
      "    (id string, a string, b string, c string)\n",
      " stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n",
      " with serdeproperties (\"hbase.columns.mapping\"=\"d:a,d:b,d:c\")\n",
      " tblproperties(\"hbase.table.name\" = \"hive_test_hbase\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建hive外接HBase的表，外接表需要使用pyhive来创建\n",
    "from pyhive import hive\n",
    "cursor = hive.connect(\"localhost\").cursor()\n",
    "hive_test_hbase = \"\"\"\n",
    " create external table if not exists hive_test.hive_test_hbase\n",
    "    (id string, a string, b string, c string)\n",
    " stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n",
    " with serdeproperties (\"hbase.columns.mapping\"=\":key,d:a,d:b,d:c\")\n",
    " tblproperties(\"hbase.table.name\" = \"hive_test_hbase\")\n",
    "\"\"\"\n",
    "print(hive_test_hbase)\n",
    "cursor.execute(hive_test_hbase)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'd': {'name': b'd:', 'max_versions': 1, 'compression': b'NONE', 'in_memory': False, 'bloom_filter_type': b'NONE', 'bloom_filter_vector_size': 0, 'bloom_filter_nb_hashes': 0, 'block_cache_enabled': False, 'time_to_live': 2147483647}}\n"
     ]
    }
   ],
   "source": [
    "# 尝试向HBase写入数据\n",
    "import random\n",
    "table = conn.table(\"hive_test_hbase\")\n",
    "print(table.families())\n",
    "for id in range(20):\n",
    "    table.put(row=str(id),\n",
    "              data={\"d:a\":str(random.randint(1,100)),\n",
    "                    \"d:b\":str(random.randint(100,200)),\n",
    "                    \"d:c\":str(random.randint(200,300))},\n",
    "              timestamp=1,\n",
    "              wal=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 d:a: 100,d:b: 188,d:c: 298\n",
      "1 d:a: 13,d:b: 166,d:c: 250\n",
      "10 d:a: 37,d:b: 190,d:c: 255\n",
      "11 d:a: 66,d:b: 162,d:c: 219\n",
      "12 d:a: 6,d:b: 191,d:c: 290\n",
      "13 d:a: 40,d:b: 126,d:c: 239\n",
      "14 d:a: 33,d:b: 194,d:c: 257\n",
      "15 d:a: 74,d:b: 145,d:c: 226\n",
      "16 d:a: 84,d:b: 133,d:c: 275\n",
      "17 d:a: 41,d:b: 166,d:c: 214\n",
      "18 d:a: 94,d:b: 172,d:c: 249\n",
      "19 d:a: 29,d:b: 150,d:c: 223\n",
      "2 d:a: 90,d:b: 119,d:c: 228\n",
      "3 d:a: 62,d:b: 181,d:c: 224\n",
      "4 d:a: 45,d:b: 153,d:c: 253\n",
      "5 d:a: 43,d:b: 182,d:c: 277\n",
      "6 d:a: 89,d:b: 146,d:c: 243\n",
      "7 d:a: 3,d:b: 190,d:c: 268\n",
      "8 d:a: 77,d:b: 121,d:c: 297\n",
      "9 d:a: 90,d:b: 115,d:c: 286\n"
     ]
    }
   ],
   "source": [
    "# 使用hbase读数据\n",
    "for row,val in table.scan():\n",
    "    val_str = \",\".join([ (c.decode()+\": \"+val[c].decode()) for c in val ])\n",
    "    print(str(row,encoding=\"utf8\"),val_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', '100', '188', '298')\n",
      "('1', '13', '166', '250')\n",
      "('10', '37', '190', '255')\n",
      "('11', '66', '162', '219')\n",
      "('12', '6', '191', '290')\n",
      "('13', '40', '126', '239')\n",
      "('14', '33', '194', '257')\n",
      "('15', '74', '145', '226')\n",
      "('16', '84', '133', '275')\n",
      "('17', '41', '166', '214')\n",
      "('18', '94', '172', '249')\n",
      "('19', '29', '150', '223')\n",
      "('2', '90', '119', '228')\n",
      "('3', '62', '181', '224')\n",
      "('4', '45', '153', '253')\n",
      "('5', '43', '182', '277')\n",
      "('6', '89', '146', '243')\n",
      "('7', '3', '190', '268')\n",
      "('8', '77', '121', '297')\n",
      "('9', '90', '115', '286')\n"
     ]
    }
   ],
   "source": [
    "# 使用hive读数据\n",
    "cursor.execute(\"select * from hive_test.hive_test_hbase\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/03 22:43:06 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "21/12/03 22:43:08 WARN mapreduce.TableInputFormatBase: You are using an HTable instance that relies on an HBase-managed Connection. This is usually due to directly creating an HTable, which is deprecated. Instead, you should create a Connection object and then request a Table instance from it. If you don't need the Table instance for your own use, you should instead use the TableInputFormatBase.initalizeTable method directly.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id|  a|  b|  c|\n",
      "+---+---+---+---+\n",
      "|  0|100|188|298|\n",
      "|  1| 13|166|250|\n",
      "| 10| 37|190|255|\n",
      "| 11| 66|162|219|\n",
      "| 12|  6|191|290|\n",
      "| 13| 40|126|239|\n",
      "| 14| 33|194|257|\n",
      "| 15| 74|145|226|\n",
      "| 16| 84|133|275|\n",
      "| 17| 41|166|214|\n",
      "| 18| 94|172|249|\n",
      "| 19| 29|150|223|\n",
      "|  2| 90|119|228|\n",
      "|  3| 62|181|224|\n",
      "|  4| 45|153|253|\n",
      "|  5| 43|182|277|\n",
      "|  6| 89|146|243|\n",
      "|  7|  3|190|268|\n",
      "|  8| 77|121|297|\n",
      "|  9| 90|115|286|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 用spark读这个表的数据\n",
    "ss.sql(\"select * from hive_test.hive_test_hbase\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cursor.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ss.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}