{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'3.1.2'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4510/3029761203.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mfindspark\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mfindspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"yarn\", \"test\")\n",
    "lines = sc.parallelize(range(10))\n",
    "result = lines.map(lambda x: x*2)\n",
    "for x in result.items():\n",
    "    print (x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pymongo\n",
    "pymongo.__version__\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mgconn = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mgconn.list_database_names()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_db = mgconn[\"test\"]\n",
    "# 创建集合\n",
    "test_col = test_db[\"test0\"]\n",
    "# 清空集合\n",
    "test_col.drop()\n",
    "# 写json\n",
    "for k in range(10):\n",
    "    test_json = {\"_id\":k, \"name\":\"name-\"+str(k)}\n",
    "    test_col.replace_one(test_json,test_json,upsert=True)\n",
    "# 读json\n",
    "print(test_col.estimated_document_count())\n",
    "# 排序json\n",
    "for doc in test_col.find().sort(\"name\", direction=pymongo.DESCENDING):\n",
    "    print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# activemq\n",
    "# Send a Message to an Apache ActiveMQ Queue\n",
    "import stomp\n",
    "conn = stomp.Connection10()\n",
    "conn.start()\n",
    "conn.connect()\n",
    "conn.send('test_queue', 'aaa')\n",
    "conn.disconnect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "class SampleListener(object):\n",
    "    def on_message(self, headers, msg):\n",
    "        print(msg)\n",
    "\n",
    "conn = stomp.Connection10()\n",
    "conn.set_listener('SampleListener', SampleListener())\n",
    "conn.start()\n",
    "conn.connect()\n",
    "conn.subscribe('test_queue')\n",
    "time.sleep(10)\n",
    "conn.disconnect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kafka\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers='192.168.17.150:9092')\n",
    "producer.send('test_init', key=\"key-0\", value=\"aaa\", partition=0)\n",
    "producer.send('test_init', key=\"key-0\", value=\"bbb\", partition=0)\n",
    "producer.send('test_init', key=\"key-0\", value=\"ccc\", partition=0)\n",
    "producer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "consumer = KafkaConsumer('test_rhj', bootstrap_servers=['192.168.17.150:9092'])\n",
    "for msg in consumer:\n",
    "    recv = \"%s:%d:%d: key=%s value=%s\" % (msg.topic, msg.partition, msg.offset, msg.key, msg.value)\n",
    "    print(recv)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SampleListener(object):\n",
    "    def on_message(self, headers, msg):\n",
    "        print(msg)\n",
    "\n",
    "conn = stomp.Connection10()\n",
    "\n",
    "conn.set_listener('SampleListener', SampleListener())\n",
    "\n",
    "conn.start()\n",
    "\n",
    "conn.connect()\n",
    "\n",
    "conn.subscribe('SampleQueue')\n",
    "\n",
    "time.sleep(1) # secs\n",
    "\n",
    "conn.disconnect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_db = mgconn[\"test\"]\n",
    "# 创建集合\n",
    "test_col = test_db[\"test0\"]\n",
    "# 清空集合\n",
    "test_col.drop()\n",
    "# 写json\n",
    "for k in range(10):\n",
    "    test_json = {\"_id\":k, \"name\":\"name-\"+str(k)}\n",
    "    test_col.replace_one(test_json,test_json,upsert=True)\n",
    "# 读json\n",
    "print(test_col.estimated_document_count())\n",
    "# 排序json\n",
    "for doc in test_col.find().sort(\"name\", direction=pymongo.DESCENDING):\n",
    "    print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_db = mgconn[\"test\"]\n",
    "# 创建集合\n",
    "test_col = test_db[\"test0\"]\n",
    "# 清空集合\n",
    "test_col.drop()\n",
    "# 写json\n",
    "for k in range(10):\n",
    "    test_json = {\"_id\":k, \"name\":\"name-\"+str(k)}\n",
    "    test_col.replace_one(test_json,test_json,upsert=True)\n",
    "# 读json\n",
    "print(test_col.estimated_document_count())\n",
    "# 排序json\n",
    "for doc in test_col.find().sort(\"name\", direction=pymongo.DESCENDING):\n",
    "    print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_db = mgconn[\"test\"]\n",
    "# 创建集合\n",
    "test_col = test_db[\"test0\"]\n",
    "# 清空集合\n",
    "test_col.drop()\n",
    "# 写json\n",
    "for k in range(10):\n",
    "    test_json = {\"_id\":k, \"name\":\"name-\"+str(k)}\n",
    "    test_col.replace_one(test_json,test_json,upsert=True)\n",
    "# 读json\n",
    "print(test_col.estimated_document_count())\n",
    "# 排序json\n",
    "for doc in test_col.find().sort(\"name\", direction=pymongo.DESCENDING):\n",
    "    print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/25 13:30:13 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4510/2327735860.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"yarn\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"test\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mlines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/zzti/libs/conda3/envs/spark/lib/python3.9/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    144\u001B[0m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0m\u001B[1;32m    147\u001B[0m                           conf, jsc, profiler_cls)\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/zzti/libs/conda3/envs/spark/lib/python3.9/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_do_init\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    222\u001B[0m         \u001B[0;31m# data via a socket.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m         \u001B[0;31m# scala's mangled names w/ $ in them require special treatment.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 224\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_encryption_enabled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misEncryptionEnabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    225\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menviron\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"SPARK_AUTH_SOCKET_TIMEOUT\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    226\u001B[0m             \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetPythonAuthSocketTimeout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/zzti/libs/conda3/envs/spark/lib/python3.9/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1533\u001B[0m                     answer, self._gateway_client, self._fqn, name)\n\u001B[1;32m   1534\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1535\u001B[0;31m             raise Py4JError(\n\u001B[0m\u001B[1;32m   1536\u001B[0m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001B[1;32m   1537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mPy4JError\u001B[0m: org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"yarn\", \"test\")\n",
    "lines = sc.parallelize(range(10))\n",
    "result = lines.map(lambda x: x*2)\n",
    "for x in result.items():\n",
    "    print (x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}